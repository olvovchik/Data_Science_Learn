{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e87ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт библиотек\n",
    "import os\n",
    "import pandas as pd\n",
    "from ydata_profiling import ProfileReport\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats \n",
    "from sklearn.preprocessing import QuantileTransformer, PowerTransformer, RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "\n",
    "# многострочный вывод без использования print\n",
    "from IPython.core.interactiveshell import InteractiveShell \n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90895841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804d8e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определим пути откуда будет загружать файлы\n",
    "current_path = os.getcwd()\n",
    "file_path1 = current_path+'\\content\\Composites\\X_bp.xlsx'\n",
    "file_path2 = current_path+'\\content\\Composites\\X_nup.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106c16f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем первый файл\n",
    "df1 = pd.read_excel(file_path1, index_col = 0) \n",
    "df1.shape \n",
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f6ab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем второй файл\n",
    "df2 = pd.read_excel(file_path2, index_col = 0)\n",
    "df2.shape\n",
    "df2.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0005bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединение таблиц \n",
    "df_united = df1.join(df2, how='inner') \n",
    "df_united.shape\n",
    "df_united.sample(5).T\n",
    "# '''\n",
    "# Таблицы имели разную размерность, но при объединении с типом inner: \n",
    "#     было выполнено внутреннее соединение - объединились только строки, имеющие одинаковый индекс  \n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6295561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поменяем название одного столбца, сделаем с заглавной)) \n",
    "df_united.rename(columns = {'модуль упругости, ГПа':'Модуль упругости, ГПа'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058c3aec",
   "metadata": {},
   "source": [
    "# Первичный анализ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6d7b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# На всякий случай удалим дубликаты (если такие имеются)\n",
    "df_united.drop_duplicates(inplace = True)\n",
    "df_united.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44c3af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим суммарную информацию \n",
    "df_united.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274cce45",
   "metadata": {},
   "source": [
    "## Описание столбцов\n",
    "\n",
    "    Соотношение матрица-наполнитель       - Цель № 3      \n",
    "    Плотность, кг/м3                      - характеристика матрицы   \n",
    "    Модуль упругости, ГПа                 - характеристика матрицы \n",
    "    Количество отвердителя, м.%           - характеристика матрицы \n",
    "    Содержание эпоксидных групп,%_2       - характеристика матрицы \n",
    "    Температура вспышки, С_2              - характеристика матрицы  \n",
    "    Поверхностная плотность, г/м2         - характеристика матрицы \n",
    "    Модуль упругости при растяжении, ГПа  - Цель № 1 \n",
    "    Прочность при растяжении, МПа         - Цель № 2 \n",
    "    Потребление смолы, г/м2               - характеристика наполнителя\n",
    "    Угол нашивки, град                    - характеристика наполнителя     \n",
    "    Шаг нашивки                           - характеристика наполнителя   \n",
    "    Плотность нашивки                     - характеристика наполнителя    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e549fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Дополнительно проверим на пустые значения \n",
    "df_united.isnull().any() \n",
    "# df.isnull().count() \n",
    "# df.isna().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6182c7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверим уникальные значения по столбцам\n",
    "df_united.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34613785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выведем описательные статистические данные\n",
    "df_united.describe(include = 'all').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7954ab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим моду\n",
    "df_united.mode().iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bd53ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим медиану\n",
    "df_united.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb2dd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавим моду и медиану к статданным  \n",
    "def get_df_stat(dataset):\n",
    "    df_stat_1 = dataset.describe()\n",
    "    num_col = [i for i in dataset.columns if dataset[i].dtype in ['int64','float64']]\n",
    "    df_stat_2 = pd.DataFrame(dataset[num_col].median()).T.rename(index = {0:'median'})\n",
    "    df_stat_3 = pd.DataFrame(dataset[num_col].mode().iloc[0]).T.rename(index = {0:'mode'})\n",
    "    df_stat = pd.concat([df_stat_1, df_stat_2, df_stat_3])\n",
    "    return df_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6616cf78",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_df_stat(df_united).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdec2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим корреляционную матрицу \n",
    "df_united.corr(method = 'pearson') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6031cd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализируем корреляционную матрицу \n",
    "sns.heatmap(df_united.corr(method = 'pearson'), \n",
    "            annot=True, \n",
    "            fmt ='.1f', \n",
    "            cmap= 'coolwarm',\n",
    "            linewidths=0.1, \n",
    "            linecolor='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e6cece",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Выведем графики распределения каждой из переменной, попарные графики рассеяния точек\n",
    "sns.pairplot(data = df_united, kind = 'scatter', diag_kind = 'kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60a1b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сделаем функцию для вывоода графиков: histplot, boxplot, Q-Q\n",
    "def draw_diagram (dataset, column, stat_dataset = None):\n",
    "    fig = plt.figure(figsize = (20,5))\n",
    "    \n",
    "    Q1 = dataset[column].quantile(0.25)\n",
    "    Q3 = dataset[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    Qmin = (Q1 - 1.5 * IQR)\n",
    "    Qmax = (Q3 + 1.5 * IQR)\n",
    "    skew = stats.skew(dataset[column], axis=0, bias=True)\n",
    "    \n",
    "    # гистограмма распределения\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.histplot(x = dataset[column], kde = True, color = 'orange')\n",
    "    \n",
    "    if stat_dataset is not None:\n",
    "        plt.axvline(stat_dataset.loc['mean',column], \n",
    "                    c ='red', label = 'mean', \n",
    "                    lw = 2, ls = '--', ymax = 0.85)\n",
    "        plt.axvline(stat_dataset.loc['median',column], \n",
    "                    c = 'blue', label = 'median', \n",
    "                    lw = 2, ls = '-', ymax = 0.85)\n",
    "        plt.axvline(stat_dataset.loc['mode',column], \n",
    "                    c = 'green', label = 'mode', \n",
    "                    lw = 2, ls = '--', ymax = 0.85)\n",
    "        plt.legend(loc='best')\n",
    "    plt.axvline(Qmin, \n",
    "                c ='black', label = 'Q1 - 1.5 * IQR', \n",
    "                lw = 2, ls = '-', ymin = 0, ymax = 0.05)   \n",
    "    plt.axvline(Q1, \n",
    "                c ='black', label = 'Q1', \n",
    "                lw = 3, ls = '-', ymin = 0, ymax = 0.1)\n",
    "    plt.axvline(Q3, \n",
    "                c ='black', label = 'Q3', \n",
    "                lw = 3, ls = '-', ymin = 0, ymax = 0.1)\n",
    "    plt.axvline(Qmax, \n",
    "                c ='black', label = 'Q3 + 1.5 * IQR', \n",
    "                lw = 2, ls = '-', ymin = 0, ymax = 0.05)\n",
    "    plt.legend(loc='best')\n",
    "    plt.title(f'skew = {round(skew,3)}')\n",
    "    plt.xlabel(None)\n",
    "    plt.ylabel(None)    \n",
    "    \n",
    "    # ящик с усами\n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.boxplot(x = dataset[column], orient = \"h\", color = 'chartreuse') \n",
    "    plt.title(column)\n",
    "    plt.xlabel(None)\n",
    "\n",
    "    # график Q-Q\n",
    "    plt.subplot(1, 3, 3)\n",
    "    stats.probplot(x = dataset[column], dist=\"norm\", plot=plt) \n",
    "    plt.title(None)\n",
    "    plt.xlabel(None)\n",
    "    plt.ylabel(None)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867e629c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in df_united.columns:\n",
    "    if df_united[i].dtype in ['int64', 'float64']:       \n",
    "        draw_diagram(df_united, i, get_df_stat(df_united))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f621c08",
   "metadata": {},
   "source": [
    "По графикам видим:\n",
    " - наличие выбросов\n",
    " - распеределение близкое к нормальному по всем параметрам, параметр \"Поверхностная плотность, г/м2\" имеет положительную ассиметрию "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0895c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим подробные отчеты с помощью ProfileReport\n",
    "ProfileReport(df_united)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fcdbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# Параметр 'Угол нашивки, град' преобразуем в категориальный признак или нет?\n",
    "# По данным из имеющегося датасета данный параметр принимает только два значения.\n",
    "# Но: скорее всего градус угла может быть отличным от указанных значений \n",
    "# '''\n",
    "\n",
    "# Поменяем type\n",
    "df_united_cat = df_united.copy()\n",
    "df_united_cat['Угол нашивки, град'] = df_united_cat['Угол нашивки, град'].astype('category')\n",
    "\n",
    "# Кодирование категорий с помощью skclearn OneHotEncoder\n",
    "df_united_OHE = df_united.copy()\n",
    "OHE = OneHotEncoder()\n",
    "OHE = OHE.fit(df_united_OHE[['Угол нашивки, град']])\n",
    "df_united_OHE = df_united_OHE.join(pd.DataFrame(OHE.transform(df_united_OHE[['Угол нашивки, град']]).toarray()))\n",
    "df_united_OHE.drop('Угол нашивки, град', axis = 1, inplace = True)\n",
    "\n",
    "# Кодирование категорий с помощью Pandas_get_dummies\n",
    "df_united_GD = df_united.copy()\n",
    "df_united_GD = pd.get_dummies(df_united_GD, columns=['Угол нашивки, град'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e306b46e",
   "metadata": {},
   "source": [
    "# Борьба с выбросами"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2793d372",
   "metadata": {},
   "source": [
    "\n",
    "#### Каким способом будем бороться с выбросами:\n",
    " 1. Удалим используя правила трех сигм - '3sig'\n",
    " 2. Удалим используя межквартильный размах - 'IQR'\n",
    " 3. Удалим используя 5% и 95% квантилей - '5Q95'\n",
    " 4. Оставим\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f640b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция удаления выбросов\n",
    "def drop_blowout (dataset, drop_method = None, exp_col = None):\n",
    "    \n",
    "    if drop_method is not None:      \n",
    "        print (f'Метод удаления - {drop_method}')\n",
    "        if exp_col is not None:  \n",
    "            num_col = [i for i in dataset.columns if dataset[i].dtype in ['int64','float64'] and i not in exp_col]\n",
    "        else:\n",
    "            num_col = [i for i in dataset.columns if dataset[i].dtype in ['int64','float64']]        \n",
    "        index_drop = []\n",
    "        \n",
    "        for col_name in num_col:             \n",
    "            \n",
    "            if drop_method == '3sig':\n",
    "                border_low = dataset[col_name].mean() - (3 * dataset[col_name].std())\n",
    "                border_up = dataset[col_name].mean() + (3 * dataset[col_name].std())\n",
    "                \n",
    "            elif drop_method == 'IQR':\n",
    "                IQR = dataset[col_name].quantile(0.75) - dataset[col_name].quantile(0.25)\n",
    "                border_low = dataset[col_name].quantile(0.25) - (1.5 * IQR)\n",
    "                border_up = dataset[col_name].quantile(0.75) + (1.5 * IQR)   \n",
    "                \n",
    "            elif drop_method == '5Q95':\n",
    "                border_low = dataset[col_name].quantile(0.05)\n",
    "                border_up = dataset[col_name].quantile(0.95)\n",
    "\n",
    "            index_line = dataset[(dataset[col_name] < border_low) | (dataset[col_name] > border_up)].index  \n",
    "            index_drop.extend(index_line)\n",
    "            print(f'Для параметра: \"{col_name}\": удалено {len(index_line)} строк с индексами {list(index_line)}') \n",
    "            \n",
    "        print(f'Будет удалено {len(list(set(index_drop)))} строк с индексами {set(index_drop)}')\n",
    "        dataset_new = dataset.drop(index = list(set(index_drop)), axis = 0) \n",
    "        print(f'Новый размер = {dataset_new.shape}')\n",
    "        return dataset_new\n",
    "    else: \n",
    "        return dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa56dd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3sig = drop_blowout(df_united_cat, '3sig')\n",
    "print()\n",
    "df_3sig = drop_blowout(df_3sig, '3sig')\n",
    "# print()\n",
    "# df_3sig = drop_blowout(df_3sig, '3sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2e3fce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in df_3sig.columns:\n",
    "    if df_3sig[i].dtype in ['int64', 'float64']:       \n",
    "        draw_diagram(df_3sig, i, get_df_stat(df_3sig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10531462",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_IQR = drop_blowout(df_united_cat, 'IQR') \n",
    "print()\n",
    "df_IQR = drop_blowout(df_IQR, 'IQR')\n",
    "print()\n",
    "df_IQR = drop_blowout(df_IQR, 'IQR')\n",
    "# print()\n",
    "# df_IQR = drop_blowout(df_IQR, 'IQR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6055eb89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in df_IQR.columns:\n",
    "    if df_IQR[i].dtype in ['int64', 'float64']:       \n",
    "        draw_diagram(df_IQR, i, get_df_stat(df_IQR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b981d6f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_5Q95 = drop_blowout(df_united_cat, '5Q95')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277a1da4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in df_5Q95.columns:\n",
    "    if df_5Q95[i].dtype in ['int64', 'float64']:       \n",
    "        draw_diagram(df_5Q95, i, get_df_stat(df_5Q95))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae438e1b",
   "metadata": {},
   "source": [
    "\n",
    "#### Что получили:  \n",
    " 1. Используя правила трех сигм - '3sig' - удалили 23 строки, при повторных итерациях еще 4 \n",
    " 2. Используя межквартильный размах - 'IQR'- удалии 87 строк, при повторных итерациях еще 14\n",
    " 3. Используя 5% и 95% квантилей - '5Q95' - удалили 727 строк (пока пропустим данный метод по причине большой потери данных + прослеживается некая закономерность в подмене данных, нужно вернуться к данному моменту позже)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab20463",
   "metadata": {},
   "source": [
    "# Преобразование данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd87d50",
   "metadata": {},
   "source": [
    "\n",
    "#### Каким способом будет преобразовывать данные: \n",
    " 1. StandardScaler - для датасетов с удалением выбросов, кроме распределений с явной ассиметрией \n",
    "    1.  PowerTransformer - для датасетов с удалением выбросов и распределений с явной ассиметрией  \n",
    " 2. MinMaxScaler - для датасетов с удалением выбросов\n",
    " 3. RobustScaler, QuantileTransformer - для датасетов без удаления выбросов \n",
    " \n",
    "Целевые переменные трогать не будем\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6ce9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Попробуем сделать логарифмическое преобразование \"Поверхностная плотность, г/м2\"\n",
    "df_norm_lg = df_united.copy()\n",
    "df_norm_lg['Поверхностная плотность, г/м2'] = np.log(df_norm_lg['Поверхностная плотность, г/м2'])\n",
    "draw_diagram(df_norm_lg, 'Поверхностная плотность, г/м2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f56936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Попробуем сделать преобразование \"Поверхностная плотность, г/м2\" через kвадратный корень \n",
    "df_norm_sqrt = df_united.copy()\n",
    "df_norm_sqrt['Поверхностная плотность, г/м2'] = df_norm_sqrt['Поверхностная плотность, г/м2']**(1/2)\n",
    "draw_diagram(df_norm_sqrt, 'Поверхностная плотность, г/м2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f600469e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Попробуем сделать преобразование \"Поверхностная плотность, г/м2\" через возведение в степень \n",
    "df_norm_power = df_united.copy()\n",
    "df_norm_power['Поверхностная плотность, г/м2'] = df_norm_power['Поверхностная плотность, г/м2']**2\n",
    "draw_diagram(df_norm_power, 'Поверхностная плотность, г/м2',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b643053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Попробуем сделать преобразование с помощью skclearn PowerTransformer методом Йео-Джонсона\n",
    "PowerTranYJ = PowerTransformer(method = 'yeo-johnson', standardize = False)\n",
    "df_norm_PTYJ = df_united.copy()\n",
    "df_norm_PTYJ['Поверхностная плотность, г/м2'] = PowerTranYJ.fit_transform(np.ravel(df_norm_PTYJ['Поверхностная плотность, г/м2']).reshape(-1, 1))\n",
    "draw_diagram(df_norm_PTYJ, 'Поверхностная плотность, г/м2', get_df_stat(df_norm_PTYJ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aed70e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Попробуем сделать преобразование с помощью scipy методом Йео-Джонсона\n",
    "df_norm_sc_YJ = df_united.copy()\n",
    "df_norm_sc_YJ['Поверхностная плотность, г/м2'], paramyj = stats.yeojohnson(df_norm_sc_YJ['Поверхностная плотность, г/м2'])\n",
    "draw_diagram(df_norm_sc_YJ, 'Поверхностная плотность, г/м2', get_df_stat(df_norm_sc_YJ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39ffdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Попробуем сделать преобразование с помощью skclearn PowerTransformer методом Бокса-Кокса\n",
    "PowerTranBC = PowerTransformer(method = 'box-cox', standardize = False)\n",
    "df_norm_PTBC = df_united.copy()\n",
    "df_norm_PTBC['Поверхностная плотность, г/м2'] = PowerTranBC.fit_transform(np.ravel(df_norm_PTBC['Поверхностная плотность, г/м2']).reshape(-1, 1))\n",
    "draw_diagram(df_norm_PTBC, 'Поверхностная плотность, г/м2', get_df_stat(df_norm_PTBC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13f934d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Попробуем сделать преобразование с помощью scipy методом Бокса-Кокса\n",
    "df_norm_sc_BC = df_united.copy()\n",
    "df_norm_sc_BC['Поверхностная плотность, г/м2'], parambc = stats.boxcox(df_norm_sc_BC['Поверхностная плотность, г/м2'])\n",
    "draw_diagram(df_norm_sc_BC, 'Поверхностная плотность, г/м2', get_df_stat(df_norm_sc_BC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa09ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Попробуем сделать преобразование с помощью skclearn QuantileTransformer\n",
    "QuanTran = QuantileTransformer(output_distribution='normal')\n",
    "df_norm_QT = df_united.copy()\n",
    "df_norm_QT['Поверхностная плотность, г/м2'] = QuanTran.fit_transform(np.ravel(df_norm_QT['Поверхностная плотность, г/м2']).reshape(-1, 1))\n",
    "draw_diagram(df_norm_QT, 'Поверхностная плотность, г/м2', get_df_stat(df_norm_QT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e795edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оперделим целевые переменные, чтоб исключить из нормализации \n",
    "target_col = ['Соотношение матрица-наполнитель','Модуль упругости при растяжении, ГПа','Прочность при растяжении, МПа'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee61413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция преобразования\n",
    "def dataset_normal(dataset, normal_method, exp_col = None, pw_col = None):\n",
    "    \n",
    "    dataset_normal = dataset.copy()\n",
    "    \n",
    "    if exp_col is not None:\n",
    "        num_col = [i for i in df_united_cat.columns if df_united_cat[i].dtype in ['int64','float64'] and i not in exp_col]\n",
    "    else:\n",
    "        num_col = [i for i in df_united_cat.columns if df_united_cat[i].dtype in ['int64','float64']]\n",
    "    \n",
    "    if normal_method == 'MinMaxScaler':\n",
    "        MMS = MinMaxScaler()\n",
    "        dataset_normal[num_col] = MMS.fit_transform(dataset_normal[num_col])\n",
    "\n",
    "    elif normal_method == 'RobustScaler':\n",
    "        RS = RobustScaler()\n",
    "        dataset_normal[num_col] = RS.fit_transform(dataset_normal[num_col])\n",
    "\n",
    "    elif normal_method == 'QuantileTransformer':\n",
    "        QT = QuantileTransformer(output_distribution='normal', n_quantiles = len(dataset))\n",
    "        dataset_normal[num_col] = QT.fit_transform(dataset_normal[num_col])\n",
    "    \n",
    "    elif normal_method == 'StandardScaler':\n",
    "            \n",
    "        if pw_col is not None:\n",
    "            num_col.remove(pw_col)\n",
    "            PT = PowerTransformer(method = 'yeo-johnson')\n",
    "            dataset_normal[pw_col] = PT.fit_transform(np.ravel(dataset_normal[pw_col]).reshape(-1, 1))            \n",
    "        SC = StandardScaler()\n",
    "        dataset_normal[num_col] = SC.fit_transform(dataset_normal[num_col])   \n",
    "\n",
    "    return dataset_normal               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2118f7f0",
   "metadata": {},
   "source": [
    "\n",
    "#### Текущие датасеты\n",
    " 0. df_united     - первичный датасет \n",
    " 1. df_united_cat - датасет, параметр \"Угол нашивки\" переведен в категориальный\n",
    " 2. df_3sig       - датасет с удаление выбросов с помощью правил 3 сигм\n",
    " 3. df_IQR        - датасет с удаление выбросов с помощью межквартильного размаха\n",
    " 4. df_5Q95       - датасет с удаление выбросов с помощью 5% и 95% квантилей\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2a094",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dataset = [] # будем хранит список наших датасетов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c997aee7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_3sig_StandardScaler = dataset_normal(df_3sig, 'StandardScaler', target_col, 'Поверхностная плотность, г/м2')\n",
    "for i in df_3sig_StandardScaler.columns:\n",
    "    if df_3sig_StandardScaler[i].dtype in ['int64', 'float64']:       \n",
    "        draw_diagram(df_3sig_StandardScaler, i, get_df_stat(df_3sig_StandardScaler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed7ecae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3sig_StandardScaler.index.name = '3sig_SC'\n",
    "list_dataset.append(df_3sig_StandardScaler)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788ecdfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_3sig_MinMaxScaler = dataset_normal(df_3sig, 'MinMaxScaler', target_col)\n",
    "for i in df_3sig_MinMaxScaler.columns:\n",
    "    if df_3sig_MinMaxScaler[i].dtype in ['int64', 'float64']:       \n",
    "        draw_diagram(df_3sig_MinMaxScaler, i, get_df_stat(df_3sig_MinMaxScaler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7ceb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3sig_MinMaxScaler.index.name = '3sig_MMS'\n",
    "list_dataset.append(df_3sig_MinMaxScaler)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb98c4bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_IQR_StandardScaler = dataset_normal(df_IQR, 'StandardScaler', target_col, 'Поверхностная плотность, г/м2')\n",
    "for i in df_IQR_StandardScaler.columns:\n",
    "    if df_IQR_StandardScaler[i].dtype in ['int64', 'float64']:       \n",
    "        draw_diagram(df_IQR_StandardScaler, i, get_df_stat(df_IQR_StandardScaler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503ef64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_IQR_StandardScaler.index.name = 'IQR_SC'\n",
    "list_dataset.append(df_IQR_StandardScaler)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96048c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_IQR_MinMaxScaler = dataset_normal(df_IQR, 'MinMaxScaler', target_col)\n",
    "for i in df_IQR_MinMaxScaler.columns:\n",
    "    if df_IQR_MinMaxScaler[i].dtype in ['int64', 'float64']:       \n",
    "        draw_diagram(df_IQR_MinMaxScaler, i, get_df_stat(df_IQR_MinMaxScaler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36441a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_IQR_MinMaxScaler.index.name = 'IQR_MMS'\n",
    "list_dataset.append(df_IQR_MinMaxScaler)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28693164",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5Q95_StandardScaler = dataset_normal(df_5Q95, 'StandardScaler', target_col, 'Поверхностная плотность, г/м2')\n",
    "for i in df_5Q95_StandardScaler.columns:\n",
    "    if df_5Q95_StandardScaler[i].dtype in ['int64', 'float64']:       \n",
    "        draw_diagram(df_5Q95_StandardScaler, i, get_df_stat(df_5Q95_StandardScaler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b73667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5Q95_StandardScaler.index.name = '5Q95_SC'\n",
    "list_dataset.append(df_5Q95_StandardScaler) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2d8073",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5Q95_MinMaxScaler = dataset_normal(df_5Q95, 'MinMaxScaler', target_col)\n",
    "for i in df_5Q95_MinMaxScaler.columns:\n",
    "    if df_5Q95_MinMaxScaler[i].dtype in ['int64', 'float64']:       \n",
    "        draw_diagram(df_5Q95_MinMaxScaler, i, get_df_stat(df_5Q95_MinMaxScaler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d975151",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5Q95_MinMaxScaler.index.name = '5Q95_MMS'\n",
    "list_dataset.append(df_5Q95_MinMaxScaler)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162fb924",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all_RobustScaler = dataset_normal(df_united_cat, 'RobustScaler', target_col)\n",
    "for i in df_all_RobustScaler.columns:\n",
    "    if df_all_RobustScaler[i].dtype in ['int64', 'float64']:       \n",
    "        draw_diagram(df_all_RobustScaler, i, get_df_stat(df_all_RobustScaler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a497cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_RobustScaler.index.name = 'all_RS'\n",
    "list_dataset.append(df_all_RobustScaler)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146e62c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all_QuantileTransformer = dataset_normal(df_united_cat, 'QuantileTransformer', target_col)\n",
    "for i in df_all_QuantileTransformer.columns:\n",
    "    if df_all_QuantileTransformer[i].dtype in ['int64', 'float64']:       \n",
    "        draw_diagram(df_all_QuantileTransformer, i, get_df_stat(df_all_QuantileTransformer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383a5007",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_QuantileTransformer.index.name = 'all_QT'\n",
    "list_dataset.append(df_all_QuantileTransformer)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155ab569",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(list_dataset)):\n",
    "    print(list_dataset[i].index.name) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d968801",
   "metadata": {},
   "source": [
    "# Выбор датасета для работы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318fe8a3",
   "metadata": {},
   "source": [
    "#### Попробуем сделать подбор модели для решения задачи регрессии: \n",
    "    \n",
    "    1. LinearRegression - обычная линейная регрессия методом наименьших квадратов \n",
    "    2. DecisionTreeRegressor - дерево решений\n",
    "    3. RandomForestRegressor - случайный лес\n",
    "    4. LinearSVR - метод опорных векторов\n",
    "    5. KNeighborsRegressor - метод ближайших соседей \n",
    "\n",
    "Оценку будем производить с помощью среднеквадратичной ошибки и коэффициента детерминации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c74b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10045195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сделаем словарь для удобства вызова имени целевой переменной\n",
    "dict_target = {\n",
    "              'Соотношение М-Н' : 'Соотношение матрица-наполнитель', \n",
    "              'Модуль упругости' : 'Модуль упругости при растяжении, ГПа',\n",
    "              'Прочность' : 'Прочность при растяжении, МПа'\n",
    "              }\n",
    "\n",
    "# Сделаем словарь для удобства вызова модели, параметры оставим по умолчанию\n",
    "dict_model = {\n",
    "              'LinReg' : LinearRegression(), \n",
    "              'DecTreeReg' : DecisionTreeRegressor(random_state = 13),\n",
    "              'LinSVR': LinearSVR(random_state = 13),\n",
    "              'KNNReg': KNeighborsRegressor(),              \n",
    "              'RandForReg' : RandomForestRegressor(random_state = 13)\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d6d1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dataset = []\n",
    "for i in range(0,len(list_dataset)):\n",
    "    \n",
    "    X_all = list_dataset[i]\n",
    "    X_all = pd.get_dummies(X_all, columns=['Угол нашивки, град'])\n",
    "    result_target = []\n",
    "    \n",
    "    for target_name, target in dict_target.items(): \n",
    "        \n",
    "        result_model = []\n",
    "        y = X_all[target]\n",
    "        X = X_all.drop(target_col, axis = 1)\n",
    "        X_train, X_test, y_train, y_test = train_test_split (X, y, test_size = 0.3, random_state = 13)\n",
    "        \n",
    "        for model_name, model in dict_model.items():           \n",
    "            model = model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            result_model.append([list_dataset[i].index.name, target_name, model_name, mae, mse, r2])  \n",
    "        else:\n",
    "            result_model = pd.DataFrame(result_model, columns = ['Dataset', 'Target', 'Model', 'MAE', 'MSE', 'R2']) \n",
    "            result_target.append(result_model)\n",
    "    else:\n",
    "        result_dataset.append(result_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985b0f91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Поличили оценки прогноза каждой модели по каждому датасету\n",
    "for i in range(0,len(list_dataset)):\n",
    "    for g in range(0,len(target_col)):\n",
    "        result_dataset[i][g]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea4b1a8",
   "metadata": {},
   "source": [
    "#### Выводы:\n",
    "\n",
    "1 Для датасета (удаление выбросов - правило 3 сигм, преобразование - StandardScaler) - наилучшие показатели для всех трех целевых переменных у обычной линейной регрессии  \n",
    "\n",
    "2 Для датасета (удаление выбросов - правило 3 сигм, преобразование - MinMaxScaler) - наилучшие показатели для всех трех целевых переменных у обычной линейной регрессии\n",
    "    \n",
    "3 Для датасета (удаление выбросов - межквартильный размах, преобразование - StandardScaler) - наилучшие показатели для всех трех целевых переменных у обычной линейной регрессии \n",
    "  \n",
    "4 Для датасета (удаление выбросов - межквартильный размах, преобразование - MinMaxScaler) - наилучшие показатели для всех трех целевых переменных у обычной линейной регрессии \n",
    "    \n",
    "5 Для датасета (удаление выбросов -  5% и 95% квантилей, преобразование - StandardScaler) - наилучшие показатели для:\n",
    "- соотношение матрица-наполнитель -  метод опорных векторов и обычная линейная регрессия \n",
    "- модуль упругости и прочность - обычная линейная регрессия\n",
    "        \n",
    "6 Для датасета (удаление выбросов - 5% и 95% квантилей, преобразование - MinMaxScaler) - наилучшие показатели для всех трех целевых переменных у обычной линейной регрессии        \n",
    "    \n",
    "7 Для датасета (без удаления выбросов, преобразование - RobustScaler) - наилучшие показатели для:\n",
    "   - соотношение матрица-наполнитель -  метод опорных векторов и обычная линейная регрессия \n",
    "   - модуль упругости и прочность - обычная линейная регрессия    \n",
    "        \n",
    "8 Для датасета (без удаления выбросов, преобразование - QuantileTransformer) - наилучшие показатели для:\n",
    "   - соотношение матрица-наполнитель -  метод опорных векторов и обычная линейная регрессия     \n",
    "   - модуль упругости и прочность - обычная линейная регрессия       \n",
    "      \n",
    "Так как метод обычной регрессии показал наилучшие результаты (если можно так сказать) - можно сравнить результаты между ними, чтоб определится с каким датасетом будем работать дальше   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30695683",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dataset_itog = []\n",
    "for i in range(0,len(list_dataset)):\n",
    "    \n",
    "    X_all = list_dataset[i]\n",
    "    X_all = pd.get_dummies(X_all, columns=['Угол нашивки, град'])\n",
    "    result_target = []\n",
    "    \n",
    "    for target_name, target in dict_target.items(): \n",
    "        \n",
    "        y = X_all[target]\n",
    "        X = X_all.drop(target_col, axis = 1)\n",
    "        X_train, X_test, y_train, y_test = train_test_split (X, y, test_size = 0.3, random_state = 13)\n",
    "      \n",
    "        model = dict_model['LinReg']           \n",
    "        model_name = 'LinReg'\n",
    "        model = model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        result_dataset_itog.append([list_dataset[i].index.name, target_name, model_name, mae, mse, r2])  \n",
    "\n",
    "result_dataset_itog = pd.DataFrame(result_dataset_itog, columns = ['Dataset', 'Target', 'Model', 'MAE', 'MSE', 'R2']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0501dc1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result_dataset_itog.sort_values(by = ['Target','R2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114af685",
   "metadata": {},
   "source": [
    "#### Выводы: \n",
    "    \n",
    "Для дальнейшей работы решено взять датасет с удалением выбросов, используя межквартильный размах, и преобразованием StandardScaler \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82191fc4",
   "metadata": {},
   "source": [
    "# Обучение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19710f46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Скопируем датасет и сразу удалим выбросы\n",
    "df_work = df_united.copy()\n",
    "\n",
    "df_work['Угол нашивки, град'] = df_work['Угол нашивки, град'].astype('category') \n",
    "\n",
    "df_work = drop_blowout(df_work, 'IQR') \n",
    "print()\n",
    "df_work = drop_blowout(df_work, 'IQR')\n",
    "print()\n",
    "df_work = drop_blowout(df_work, 'IQR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e50ad7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.displot(df_work['Модуль упругости, ГПа'], kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3015d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переведем 'Прочность при растяжении, МПа' в гигапаскаль и поменяем название  \n",
    "# df_work['Прочность при растяжении, МПа'] = df_work['Прочность при растяжении, МПа'] / 1000\n",
    "# df_work.rename(columns = {'Прочность при растяжении, МПа':'Прочность при растяжении, ГПа'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adaf18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разобьем датасет на входящие и целевые переменные \n",
    "X = df_work.drop(['Модуль упругости при растяжении, ГПа', \n",
    "                  'Прочность при растяжении, МПа',\n",
    "                  'Соотношение матрица-наполнитель'], axis = 1)\n",
    "X1 = X.copy()\n",
    "\n",
    "y1 = df_work['Модуль упругости при растяжении, ГПа']\n",
    "\n",
    "X2 = X.copy()\n",
    "\n",
    "y2 = df_work['Прочность при растяжении, МПа']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b50f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разобьем датасеты на обучающую и тестовую выборки\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split (X1, y1, test_size = 0.3, random_state = 13)\n",
    "X1_train.shape, X1_test.shape, y1_train.shape, y1_test.shape\n",
    "\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split (X2, y2, test_size = 0.3, random_state = 13)\n",
    "X2_train.shape, X2_test.shape, y2_train.shape, y2_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4e3832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция получения числовых, категориальных колонок... \n",
    "def get_num_cat_pw_col(dataset):\n",
    "    \n",
    "    num_col = [i for i in dataset.columns if dataset[i].dtype in ['int64','float64'] \n",
    "               and stats.skew(dataset[i], axis=0, bias=True) < 0.3 \n",
    "               and stats.skew(dataset[i], axis=0, bias=True) > -0.3]  \n",
    "    cat_col = [i for i in dataset.columns if dataset[i].dtype in ['object','category']]\n",
    "    \n",
    "    pw_col = [i for i in dataset.columns if dataset[i].dtype in ['int64','float64'] \n",
    "              and (stats.skew(dataset[i], axis=0, bias=True) > 0.3 \n",
    "              or stats.skew(dataset[i], axis=0, bias=True) < -0.3)] \n",
    "    \n",
    "    return num_col, cat_col, pw_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dfa31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_col, cat_col, pw_col = get_num_cat_pw_col(X)\n",
    "num_col, cat_col, pw_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0408972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сделаем препроцессор данных\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "                                                ('num', StandardScaler(), num_col),\n",
    "                                                ('pw', PowerTransformer(method = 'yeo-johnson'), pw_col),\n",
    "                                                ('cat', OneHotEncoder(handle_unknown='ignore'), cat_col)                \n",
    "                                              ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc261df",
   "metadata": {},
   "source": [
    "### Модуль упругости при растяжении, ГПа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d542140",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Сделаем подбор гиперпараметров по сетке, т.к. м уже получили оценки моделей при использовании параметров по умолчанию \n",
    "\n",
    "X_search = preprocessor.fit_transform(X1_train) # нормализованный датасет для подбора гиперпараметров\n",
    "\n",
    "LinReg_parameters = {\n",
    "                     'fit_intercept' : [True, False],\n",
    "                     'n_jobs' : np.arange(1, 10),\n",
    "                     'positive' : [True, False]\n",
    "                    }\n",
    "reg = GridSearchCV(LinearRegression(), LinReg_parameters, cv = 5, scoring = scoring)\n",
    "search = reg.fit(X_search, y1_train)\n",
    "print(f'Параметры для {search.estimator} - {search.best_params_}')\n",
    "\n",
    "DecTreeReg_parameters = {\n",
    "                         'criterion' : ['squared_error', 'absolute_error'],\n",
    "                         'max_depth' : np.arange(1, 20),\n",
    "                         'random_state' : [13]\n",
    "                        }\n",
    "reg = GridSearchCV(DecisionTreeRegressor(), DecTreeReg_parameters, cv = 5, scoring = scoring)\n",
    "search = reg.fit(X_search, y1_train)\n",
    "print(f'Параметры для {search.estimator} - {search.best_params_}')\n",
    "\n",
    "LinSVR_parameters = {\n",
    "                     'epsilon' : np.arange(0, 10, 0.1),\n",
    "                     'loss' : ['epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "                     'fit_intercept' : [True, False],\n",
    "                     'random_state' : [13]\n",
    "                    }\n",
    "reg = GridSearchCV(LinearSVR(), LinSVR_parameters, cv = 5, scoring = scoring)\n",
    "search = reg.fit(X_search, y1_train)\n",
    "print(f'Параметры для {search.estimator} - {search.best_params_}')\n",
    "\n",
    "KNNReg_parameters = {\n",
    "                     'n_neighbors' : np.arange(1,300),\n",
    "                     'weights' : ['uniform', 'distance'],\n",
    "                     'p' : [1,2]\n",
    "                    }\n",
    "reg = GridSearchCV(KNeighborsRegressor(), KNNReg_parameters, cv = 5, scoring = scoring)\n",
    "search = reg.fit(X_search, y1_train)\n",
    "print(f'Параметры для {search.estimator} - {search.best_params_}')\n",
    "\n",
    "RandForReg_parameters = {\n",
    "                         'criterion' : ['squared_error', 'absolute_error'],\n",
    "                         'max_depth' : np.arange(1, 20),\n",
    "                         'random_state' : [13]\n",
    "                        }\n",
    "reg = GridSearchCV(RandomForestRegressor(), RandForReg_parameters, cv = 5, scoring = scoring)\n",
    "search = reg.fit(X_search, y1_train)\n",
    "print(f'Параметры для {search.estimator} - {search.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f454984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сделаем словарь для удобства вызова моделей с наилучшими гиперпараметрами \n",
    "dict_model_t1 = {\n",
    "                 'LinReg' : LinearRegression(fit_intercept = True, \n",
    "                                             n_jobs = 1, \n",
    "                                             positive = False), \n",
    "                 'DecTreeReg' : DecisionTreeRegressor(criterion = 'absolute_error', \n",
    "#                                                      max_depth = 1, \n",
    "                                                      random_state = 13),\n",
    "                 'LinSVR': LinearSVR(epsilon = 0.6, \n",
    "                                     fit_intercept = True, \n",
    "                                     loss = 'squared_epsilon_insensitive', \n",
    "                                     random_state = 13),\n",
    "                 'KNNReg': KNeighborsRegressor(n_neighbors = 271, \n",
    "                                               weights = 'uniform',\n",
    "                                               p = 2),              \n",
    "                 'RandForReg' : RandomForestRegressor(criterion = 'squared_error', \n",
    "#                                                      max_depth = 1, \n",
    "                                                      random_state = 13)\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef056d9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Обучим модели и сохраним результаты\n",
    "result_model1 = []\n",
    "for model_name, model in dict_model_t1.items():           \n",
    "        \n",
    "    # Соединим препроцессор и модель в Pipeline\n",
    "    regressor = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                ('model', model)],\n",
    "                         verbose=True)\n",
    "    \n",
    "    # Обучение\n",
    "    regressor.fit(X1_train, y1_train)\n",
    "    \n",
    "    # Получаем предсказания \n",
    "    y_pred = regressor.predict(X1_test)\n",
    "    \n",
    "    # Cнимаем метрики \n",
    "    mae = mean_absolute_error(y1_test, y_pred)\n",
    "    mse = mean_squared_error(y1_test, y_pred)\n",
    "    r2 = r2_score(y1_test, y_pred)\n",
    "    result_model1.append([model_name, mae, mse, r2])     \n",
    "    \n",
    "    # Визуализация работы модели\n",
    "    fig = plt.figure(figsize = (10,3))\n",
    "    ax = sns.lineplot(data = np.array(y1_test), label = 'true values', c = 'orange')\n",
    "    ax = sns.lineplot(data = y_pred, label = 'predicted values', c = 'darkgreen')\n",
    "    ax = plt.title(model_name)\n",
    "    ax = plt.legend(loc = 'best')\n",
    "    plt.show()\n",
    "    \n",
    "    # Cохранение моделей \n",
    "    pickle.dump(regressor, open(f'{current_path}\\\\content\\\\{model_name}_t1.pkl','wb'))\n",
    "\n",
    "result_model1 = pd.DataFrame(result_model1, columns = ['Model', 'MAE', 'MSE', 'R2']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57809425",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_model1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7cf359",
   "metadata": {},
   "source": [
    "### Прочность при растяжении, МПа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c27e7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сделаем подбор гиперпараметров по сетке, т.к. м уже получили оценки моделей при использовании параметров по умолчанию \n",
    "\n",
    "X_search = preprocessor.fit_transform(X2_train) # нормализованный датасет для подбора гиперпараметров\n",
    "\n",
    "scoring = make_scorer(r2_score)\n",
    "\n",
    "LinReg_parameters = {\n",
    "                     'fit_intercept' : [True, False],\n",
    "                     'n_jobs' : np.arange(1, 10),\n",
    "                     'positive' : [True, False]\n",
    "                    }\n",
    "reg = GridSearchCV(LinearRegression(), LinReg_parameters, cv = 5, scoring = scoring)\n",
    "search = reg.fit(X_search, y2_train)\n",
    "print(f'Параметры для {search.estimator} - {search.best_params_}')\n",
    "\n",
    "DecTreeReg_parameters = {\n",
    "                         'criterion' : ['squared_error', 'absolute_error'],\n",
    "                         'max_depth' : np.arange(1, 20),\n",
    "                         'random_state' : [13]\n",
    "                        }\n",
    "reg = GridSearchCV(DecisionTreeRegressor(), DecTreeReg_parameters, cv = 5, scoring = scoring)\n",
    "search = reg.fit(X_search, y2_train)\n",
    "print(f'Параметры для {search.estimator} - {search.best_params_}')\n",
    "\n",
    "LinSVR_parameters = {\n",
    "                     'epsilon' : np.arange(0, 10, 0.1),\n",
    "                     'loss' : ['epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "                     'fit_intercept' : [True, False],\n",
    "                     'random_state' : [13]\n",
    "                    }\n",
    "reg = GridSearchCV(LinearSVR(), LinSVR_parameters, cv = 5, scoring = scoring)\n",
    "search = reg.fit(X_search, y2_train)\n",
    "print(f'Параметры для {search.estimator} - {search.best_params_}')\n",
    "\n",
    "KNNReg_parameters = {\n",
    "                     'n_neighbors' : np.arange(1,300),\n",
    "                     'weights' : ['uniform', 'distance'],\n",
    "                     'p' : [1,2]\n",
    "                    }\n",
    "reg = GridSearchCV(KNeighborsRegressor(), KNNReg_parameters, cv = 5, scoring = scoring)\n",
    "search = reg.fit(X_search, y2_train)\n",
    "print(f'Параметры для {search.estimator} - {search.best_params_}')\n",
    "\n",
    "RandForReg_parameters = {\n",
    "                         'criterion' : ['squared_error', 'absolute_error'],\n",
    "                         'max_depth' : np.arange(1, 20),\n",
    "                         'random_state' : [13]\n",
    "                        }\n",
    "reg = GridSearchCV(RandomForestRegressor(), RandForReg_parameters, cv = 5, scoring = scoring)\n",
    "search = reg.fit(X_search, y2_train)\n",
    "print(f'Параметры для {search.estimator} - {search.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffe457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сделаем словарь для удобства вызова моделей с наилучшими гиперпараметрами \n",
    "dict_model_t2 = {\n",
    "                 'LinReg' : LinearRegression(fit_intercept = True, \n",
    "                                             n_jobs = 1, \n",
    "                                             positive = True), \n",
    "                 'DecTreeReg' : DecisionTreeRegressor(criterion = 'squared_error', \n",
    "#                                                      max_depth = 1, \n",
    "                                                      random_state = 13),\n",
    "                 'LinSVR': LinearSVR(epsilon = 1.8, \n",
    "                                     fit_intercept = True, \n",
    "                                     loss = 'squared_epsilon_insensitive', \n",
    "                                     random_state = 13),\n",
    "                 'KNNReg': KNeighborsRegressor(n_neighbors = 258, \n",
    "                                               weights = 'uniform',\n",
    "                                               p = 2),              \n",
    "                 'RandForReg' : RandomForestRegressor(criterion = 'squared_error', \n",
    "#                                                      max_depth = 1, \n",
    "                                                      random_state = 13)\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daaa4dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Обучим модели и сохраним результаты\n",
    "result_model2 = []\n",
    "for model_name, model in dict_model_t2.items():           \n",
    "        \n",
    "    # Соединим препроцессор и модель в Pipeline\n",
    "    regressor = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                ('model', model)],\n",
    "                         verbose=True)\n",
    "    \n",
    "    # Обучение\n",
    "    regressor.fit(X2_train, y2_train)\n",
    "    \n",
    "    # Получаем предсказания \n",
    "    y_pred = regressor.predict(X2_test)\n",
    "    \n",
    "    # Cнимаем метрики \n",
    "    mae = mean_absolute_error(y2_test, y_pred)\n",
    "    mse = mean_squared_error(y2_test, y_pred)\n",
    "    r2 = r2_score(y2_test, y_pred)\n",
    "    result_model2.append([model_name, mae, mse, r2])     \n",
    "\n",
    "    # Визуализация работы модели\n",
    "    fig = plt.figure(figsize = (10,3))\n",
    "    ax = sns.lineplot(data = np.array(y2_test), label = 'true values', c = 'orange')\n",
    "    ax = sns.lineplot(data = y_pred, label = 'predicted values', c = 'darkgreen')\n",
    "    ax = plt.title(model_name)\n",
    "    ax = plt.legend(loc = 'best')\n",
    "    plt.show()\n",
    "    \n",
    "    # Cохранение моделей \n",
    "    pickle.dump(regressor, open(f'{current_path}\\\\content\\\\{model_name}_t2.pkl','wb'))\n",
    "\n",
    "result_model2 = pd.DataFrame(result_model2, columns = ['Model', 'MAE', 'MSE', 'R2']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63f6fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5844d6d6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Проверка модели\n",
    "regressor = pickle.load(open(f'{current_path}\\\\content\\\\DecTreeReg_t2.pkl','rb'))\n",
    "y_pred = regressor.predict(X2_test)\n",
    "print ('MAE =',mean_absolute_error(y2_test, y_pred))\n",
    "print ('MSE =',mean_squared_error(y2_test, y_pred))\n",
    "print ('R2 =',r2_score(y2_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a61477b",
   "metadata": {},
   "source": [
    "# Обучение  нейронной сети\n",
    "\n",
    "###  Соотношение матрица-наполнитель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2d0c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разобьем датасет на обучающую и тестовую выборки\n",
    "X3 = df_work.drop('Соотношение матрица-наполнитель', axis = 1) \n",
    "y3 = df_work['Соотношение матрица-наполнитель']\n",
    "\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split (X3, y3, test_size = 0.3, random_state = 13)\n",
    "X3_train.shape, X3_test.shape, y3_train.shape, y3_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd34e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_col, cat_col, pw_col = get_num_cat_pw_col(X3_train)\n",
    "num_col, cat_col, pw_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41d51e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сделаем препроцессоры данных\n",
    "preprocessorNN = ColumnTransformer(transformers=[\n",
    "                                                ('num', StandardScaler(), num_col),\n",
    "                                                ('pw', PowerTransformer(method = 'yeo-johnson'), pw_col),\n",
    "                                                ('cat', OneHotEncoder(handle_unknown='ignore'), cat_col)                \n",
    "                                              ])\n",
    "num_col\n",
    "preprocessorNN2 = ColumnTransformer(transformers=[\n",
    "                                                ('num', MinMaxScaler(), num_col+pw_col),\n",
    "                                                ('cat', OneHotEncoder(handle_unknown='ignore'), cat_col)                \n",
    "                                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bacb6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessorNN = preprocessorNN.fit(X3_train)\n",
    "preprocessorNN2 = preprocessorNN2.fit(X3_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14096a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cохранение препроцессоров \n",
    "pickle.dump(preprocessorNN, open(f'{current_path}\\\\content\\\\modelNS\\\\preprocessorNN2.pkl','wb'))\n",
    "pickle.dump(preprocessorNN2, open(f'{current_path}\\\\content\\\\modelNS\\\\preprocessorNN2.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba6d25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X3_train = preprocessorNN.transform(X3_train)\n",
    "X3_test = preprocessorNN.transform(X3_test)\n",
    "# X3_train = preprocessorNN2.transform(X3_train)\n",
    "# X3_test = preprocessorNN2.transform(X3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce664b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X3_train['Угол нашивки, град'] = utils.to_categorical(X3_train['Угол нашивки, град'])\n",
    "# X3_test['Угол нашивки, град'] = utils.to_categorical(X3_test['Угол нашивки, град'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d7c1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordno = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4921690",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Начнем подбирать параметры НС\n",
    "model = None\n",
    "ordno += 1\n",
    "\n",
    "# Оптимизаторы\n",
    "adam = Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "sgd = SGD(learning_rate=0.01, momentum=0.0, nesterov=False)\n",
    "rms = RMSprop() \n",
    "\n",
    "# Модель НС\n",
    "model = Sequential()\n",
    "# model.add(BatchNormalization(input_dim = 12))\n",
    "model.add(Dense(8, input_dim = 13, kernel_initializer='normal', activation = 'relu')) \n",
    "# model.add(Dense(8, kernel_initializer='normal', activation = 'relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal', activation = 'linear'))\n",
    "\n",
    "model.compile(loss = 'mean_squared_error', \n",
    "              optimizer = adam, \n",
    "              metrics = ['mean_squared_error']) \n",
    "\n",
    "# Сохранение лучшей модели\n",
    "modelcheckpoint_list = ModelCheckpoint(f'{current_path}\\\\content\\\\modelNS\\\\Best_model{ordno}.hdf5', \n",
    "                                       monitor='val_loss', \n",
    "                                       verbose = 1, \n",
    "                                       save_best_only = True, \n",
    "                                       mode ='min')\n",
    "# Ранная остановка\n",
    "earlystop_list = EarlyStopping(monitor = 'val_loss', \n",
    "                               mode = 'min', \n",
    "                               verbose = 1, \n",
    "                               patience = 20)\n",
    "\n",
    "history = model.fit(X3_train, y3_train, \n",
    "                    batch_size = 32, \n",
    "                    epochs = 1000,\n",
    "                    shuffle = True,\n",
    "                    validation_split = 0.1,\n",
    "                    callbacks = [earlystop_list, modelcheckpoint_list],\n",
    "                    verbose = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af151e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize = (5,3))\n",
    "# ax =plt.plot(history.history['loss'], \n",
    "#          label='loss на обучающем наборе')\n",
    "# ax = plt.plot(history.history['val_loss'], \n",
    "#          label='loss на проверочном наборе')\n",
    "# ax = plt.xlabel('Эпоха обучения')\n",
    "# ax = plt.ylabel('loss')\n",
    "# ax = plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb361a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получим метрики сохраненных моделей - StandardScaler\n",
    "result_model3 = []\n",
    "for i in range(1,35):\n",
    "    saved_model = load_model(f'{current_path}\\\\content\\\\modelNS\\\\Best_model{i}.hdf5')\n",
    "    # Получаем предсказания \n",
    "    y_pred = saved_model.predict(X3_test)\n",
    "\n",
    "    # Cнимаем метрики \n",
    "    mae = mean_absolute_error(y3_test, y_pred)\n",
    "    mse = mean_squared_error(y3_test, y_pred)\n",
    "    r2 = r2_score(y3_test, y_pred)      \n",
    "    result_model3.append([f'Best_model{i}', mae, mse, r2])  \n",
    "    \n",
    "result_model3 = pd.DataFrame(result_model3, columns = ['Model', 'MAE', 'MSE', 'R2'])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94e40d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_model3.sort_values(by = ['R2']) # StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc86e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получим метрики сохраненных моделей - MinMaxScaler\n",
    "result_model4 = []\n",
    "for i in range(1,35):\n",
    "    saved_model = load_model(f'{current_path}\\\\content\\\\modelNS\\\\Best_model{i}.hdf5')\n",
    "    # Получаем предсказания \n",
    "    y_pred = saved_model.predict(X3_test)\n",
    "\n",
    "    # Cнимаем метрики \n",
    "    mae = mean_absolute_error(y3_test, y_pred)\n",
    "    mse = mean_squared_error(y3_test, y_pred)\n",
    "    r2 = r2_score(y3_test, y_pred)      \n",
    "    result_model4.append([f'Best_model{i}', mae, mse, r2])  \n",
    "    \n",
    "result_model4 = pd.DataFrame(result_model4, columns = ['Model', 'MAE', 'MSE', 'R2'])             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb4723f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_model4.sort_values(by = ['R2']) # MimMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4665f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load_model(f'{current_path}\\\\content\\\\modelNS\\\\Best_model8.hdf5')\n",
    "\n",
    "best_model.summary()\n",
    "\n",
    "# Получаем предсказания \n",
    "y_pred = best_model.predict(X3_test)\n",
    "\n",
    "# Cнимаем метрики \n",
    "mae = mean_absolute_error(y3_test, y_pred)\n",
    "mse = mean_squared_error(y3_test, y_pred)\n",
    "r2 = r2_score(y3_test, y_pred)\n",
    "mae, mse, r2  \n",
    "\n",
    "# Сохранение весов и модели в другом формате\n",
    "best_model.save_weights(f'{current_path}\\\\content\\\\modelNS\\\\Weights_final_model.h5')\n",
    "best_model.save(f'{current_path}\\\\content\\\\modelNS\\\\Final_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876a73ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация работы НС\n",
    "fig = plt.figure(figsize = (10,3))\n",
    "ax = sns.lineplot(data = np.array(y3_test), label = 'true values', c = 'orange')\n",
    "ax = sns.lineplot(data = y_pred[:,0], label = 'predicted values', c = 'darkgreen')\n",
    "ax = plt.legend(loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1356330",
   "metadata": {},
   "source": [
    "### Подбор параметров НС перебором"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed8005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_list = [1,2,3]\n",
    "neuron_list = [8,16,32,64]\n",
    "activation_list = ['tanh','linear','relu']\n",
    "optimizer_list = ['sgd','adam']\n",
    "\n",
    "for layer in layer_list:\n",
    "    \n",
    "    for neuron in neuron_list: \n",
    "        \n",
    "        for funcactiv in activation_list:\n",
    "            \n",
    "            for optim in optimizer_list:\n",
    "                \n",
    "                # Модель НС\n",
    "                model = Sequential()\n",
    "                model.add(Dense(neuron, input_dim = 13, kernel_initializer='normal', activation = funcactiv)) \n",
    "                \n",
    "                if layer > 1:                 \n",
    "                    model.add(Dense(neuron, kernel_initializer='normal', activation = funcactiv))\n",
    "                \n",
    "                if layer > 2: \n",
    "                    model.add(Dense(neuron, kernel_initializer='normal', activation = funcactiv))\n",
    "                \n",
    "                model.add(Dense(1, kernel_initializer='normal', activation = 'linear'))\n",
    "\n",
    "                model.compile(loss = 'mean_squared_error', \n",
    "                              optimizer = optim, \n",
    "                              metrics = ['mean_squared_error']) \n",
    "\n",
    "                # Сохранение лучшей модели\n",
    "                modelcheckpoint_list = ModelCheckpoint(f'{current_path}\\\\content\\\\modelNS\\\\СycleEnum3\\\\{layer}_{neuron}_{funcactiv}_{optim}.hdf5', \n",
    "                                                       monitor='val_loss', \n",
    "                                                       verbose = 0, \n",
    "                                                       save_best_only = True, \n",
    "                                                       mode ='min')\n",
    "                # Ранная остановка\n",
    "                earlystop_list = EarlyStopping(monitor = 'val_loss', \n",
    "                                               mode = 'min', \n",
    "                                               verbose = 0, \n",
    "                                               patience = 20)\n",
    "\n",
    "                history = model.fit(X3_train, y3_train, \n",
    "                                    batch_size = 32, \n",
    "                                    epochs = 1000,\n",
    "                                    shuffle = True,\n",
    "                                    validation_split = 0.1,\n",
    "                                    callbacks = [earlystop_list, modelcheckpoint_list],\n",
    "                                    verbose = 0)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474e1c0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Получим метрики сохраненных моделей - StandardScaler\n",
    "result_model5 = []\n",
    "for layer in layer_list:\n",
    "    \n",
    "    for neuron in neuron_list: \n",
    "        \n",
    "        for funcactiv in activation_list:\n",
    "            \n",
    "            for optim in optimizer_list:\n",
    "    \n",
    "                saved_model = load_model(f'{current_path}\\\\content\\\\modelNS\\\\СycleEnum\\\\{layer}_{neuron}_{funcactiv}_{optim}.hdf5')\n",
    "\n",
    "                # Получаем предсказания \n",
    "                y_pred = saved_model.predict(X3_test)\n",
    "\n",
    "                # Cнимаем метрики \n",
    "                mae = mean_absolute_error(y3_test, y_pred)\n",
    "                mse = mean_squared_error(y3_test, y_pred)\n",
    "                r2 = r2_score(y3_test, y_pred)      \n",
    "                result_model5.append([f'{layer}_{neuron}_{funcactiv}_{optim}', mae, mse, r2])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce18d562",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result_model5 = pd.DataFrame(result_model5, columns = ['Model', 'MAE', 'MSE', 'R2'])     \n",
    "result_model5.sort_values(by = ['R2']).tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f693006b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Получим метрики сохраненных моделей - MinMaxScaler\n",
    "result_model6 = []\n",
    "for layer in layer_list:\n",
    "    \n",
    "    for neuron in neuron_list: \n",
    "        \n",
    "        for funcactiv in activation_list:\n",
    "            \n",
    "            for optim in optimizer_list:\n",
    "    \n",
    "                saved_model = load_model(f'{current_path}\\\\content\\\\modelNS\\\\СycleEnum2\\\\{layer}_{neuron}_{funcactiv}_{optim}.hdf5')\n",
    "\n",
    "                # Получаем предсказания \n",
    "                y_pred = saved_model.predict(X3_test)\n",
    "\n",
    "                # Cнимаем метрики \n",
    "                mae = mean_absolute_error(y3_test, y_pred)\n",
    "                mse = mean_squared_error(y3_test, y_pred)\n",
    "                r2 = r2_score(y3_test, y_pred)      \n",
    "                result_model6.append([f'{layer}_{neuron}_{funcactiv}_{optim}', mae, mse, r2])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b17fafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_model6 = pd.DataFrame(result_model6, columns = ['Model', 'MAE', 'MSE', 'R2'])     \n",
    "result_model6.sort_values(by = ['R2']).tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9e7b42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
